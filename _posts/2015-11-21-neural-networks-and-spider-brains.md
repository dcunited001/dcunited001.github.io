---
title: "Neural Networks and Spider Brains"
categories: blog
tags: "psychology philosophy ai machine-learning"
headline: Do Spiders Dream of Electric Sheep?
author:
  name: "David Conner"
excerpt: "While certain physical neural circuit arrangements are required and
while chemistry and genetics are certainly involved to some degree, I
believe this merely sets up circuitry that will evolve to reflect
metaphysics -- *heady crystals!!* -- In other words, given the proper
initial circuitry and given a variety of similar input experiences,
the remarkably small spider's brain will inevitably evolve certain key
functionalities -- or it will not survive."
---

Sometimes, i wish i wasn't so intelligent because the most mundane
things will send my mind off into another place.  Don't get me wrong,
I enjoy philosophizing about this stuff, but it's just not useful for
me and it can be very distracting from what I actually need to do.
However, as bad as my writing can be, I feel like it's important for
me to get this stuff out there.  Occasionally, i feel like writing
could perhaps be more important than anything else I would do.
Probably not though.

## [Would A Spider Notice An Apple Dropping From A Tree?](#would-a-spider-notice-an-apple-dropping-from-a-tree)

#### &#x03D5; [Instinct and Insight](#instinct-and-insight)

#### &#x03D5; [The Origin of Instinct](#the-origin-of-instinct)

#### &#x03D5; [Of Mice and Massively Parallel Pattern Recognition](#of-mice-and-massively-parallel-pattern-recognition)

#### &#x03D5; [Phenomenization](#phenomenization)

#### &#x03D5; [Spidey Senses](#spidey-senses)

#### &#x03D5; [Cross Entropy](#cross-entropy)

#### &#x03D5; [Visual Processing, Au Naturale](#visual-processing-au-naturale)

#### &#x03D5; [Quantum Clarification](#quantum-clarification)

#### &#x03D5; [Spidey Time](#spidey-time)

#### &#x03D5; [Do Spiders Dream of Electric Sheep?](#do-spiders-dream-of-electric-sheep)

#### &#x03D5; [Ad Nihilum](#ad-nihilum)

> "This is not a *I'm sooo smart* blog.  I promise.  I hate doing
> that, I really do.  I also don't like prefacing an article with
> this." - Me, in 2015, after being subjected to a half decade of
> psychological torture.

<!-- ok, no i'm not actually that smart... i should definitely not be
     so full of myself, but i'm obviously compensating... for my life
     experience. this is and always was a huge mistake. stubborn
     rejection of my life experience in many ways was the only thing
     keeping me going. -->

<a name="would-a-spider-notice-an-apple-dropping-from-a-tree" />

# [Would A Spider Notice An Apple Dropping From A Tree?](#would-a-spider-notice-an-apple-dropping-from-a-tree)

So, a few months ago, I was messing with a spider for a few minutes.
Pretty simple, right?  I just kinda poked around it for a few minutes,
seeing how it would react to me.  And a few minutes later, I walked
past its field of vision and it cringed in fear.  As if it recognized
me -- or maybe not *me*, but at least *something like me* that it had
come to know as a possible threat.

Most people wouldn't notice this much.  Or they've long since stopped
pondering such simple things.  It's a fucking spider.  It's probably
not that intelligent, right?  These are great questions for the 5 year
old mind to mull over.

Yet, I feel obligated to ruminate over small things like this.  Or
rather, i guess i can't help it.  And because my mind is good at
making insights with limited information, with every new piece of
information it uncovers, more can be discovered. And because I have a
tendency to view the same phenomena and relationships from different
perspectives and through various lenses, I literally never stop
thinking, though my thought processes are a bit scattered.  In this
post, I'm combining inferences from biology, neurology, philosophy and
computer science.

So, even with the simplest things -- like a fucking spider on the wall
-- the wheels keep turning.

<a name="instinct-and-insight" />

### [Instinct and Insight](#instinct-and-insight)

What I found intriguing about this is a delimma biologists and
philosophers have spent much effort observing and debating.  For me,
this issue is made profoundly more entertaining when you consider the
spider's size and the fact that any single error on the spider's part
can be fatal.

> Where does instinct-based knowledge end and insight-based knowledge
> begin?

How is instinct imprinted in the spider brain?  Is it *physically*
imprinted in the arrangement of neurons?  Is it chemically imprinted
in chemicals released early on?  Is instinct encoded in the genes?
While each of these are likely each true to some degree, I believe
that instinct is more distal.

While certain physical neural circuit arrangements are required and
while chemistry and genetics are certainly involved to some degree, I
believe this merely sets up circuitry that will evolve to reflect
metaphysics -- *heady crystals!!* -- In other words, given the proper
initial circuitry and given a variety of similar input experiences,
the remarkably small spider's brain will inevitably evolve certain key
functionalities -- or it will not survive.

<a name="the-origin-of-instinct" />

### [The Origin of Instinct](#the-origin-of-instinct)

However, we are still left to ponder the origin of instinct, for which
biologists have expended centuries of effort.  What features of a
spider's brain are determined by genetics?  By chemistry?  And initial
state of neural networks?  To what degree does each factor affect the
outcome and how?

In my own opinion, I surmise that influence of genetics, while
certainly a critical dependency, is much more limited than we realize.
Genetics seems to be a critical factor for at least the following:
various types of neurons, how parent cell-types evolve into child
cell-types, flexability of networks of neurons, and the chemicals they
can use to communicate.

So while genetics is crucial, it's moreso the information flowing
through neurons that allow these networks to make inferences and
insights.  If you were to alter the input -- filtering it in some way,
adding a new sense or removing one -- the networks and circuits would
change significantly.

<a name="of-mice-and-massively-parallel-pattern-recognition" />

### [Of Mice and Massively Parallel Pattern Recognition](#of-mice-and-massively-parallel-pattern-recognition)

It never ceases to amaze me, knowing what I do about our own
capabilities for artificial intelligence.  Fifty petaflops in 2015
gets us what?  And how much electric power does that require?

So how can such a small animal with such a limited brain be capable of
such complex pattern recognition?  Not only is it capable of
recognizing moving visual input as a threat, it is capable of
*quickly* categorizing these experiences into types of threats, ** *
with NO capacity for communication! * ** Critically, a spider also
needs to be able to establish types of threats with a limited number
of experiences -- again, one experience can be fatal.

<a name="phenomenization" />

### [Phenomenization](#phenomenization)

So, because it can recognize types of threats and other types of input
experiences as necessary for its survival, then it's brain must be
capable of constructing patterns of sensory input, as well as patterns
of patterns of sensory input .. ad infinitum.

> This is why AI is computationally expensive.  Lucky us, huh? Though
> IMO, it seems that generalized AI will become less 'dangerous' the
> more highly evolved it becomes and the more processing power it
> possesses.  Oh irony...
>
> Highly evolved AI will likely begin to posses the same altruistic
> virtues that we revere, though I'm unsure of what that entails.
> Could be good, could be bad.  After all, how would an infallible,
> omniscient being with altruistic values judge the human race today?
>
> It's critical that I clarify: I am NOT equating AI with God.  Yet,
> when compared with man, AI may *seem* to have *god-like* qualities.
> Man cannot create God, which I believe to be like a force that
> pervades the Universe that transcends time and space.  For the same
> reason that you cannot count to infinity, you could not define God.
> Anything that you can name must certainly be less.  Yet, AI may
> evolve to embody similar metaphysical structures as those I might
> imagine God to embody.

Moving on.  So furthermore, in addition to pattern recognition, a
spider's brain must be capable of correlating various kinds of sensory
input and *subconsciously* abstracting them into phenomena.  It must
do so with virtually zero a priori knowledge and absolutely zero
high-level communication.

<a name="spidey-senses" />

### [Spidey Senses](#spidey-senses)

Spiders have eight eyes, which means their depth perception, object
recognition and perception of motion patterns will *all* be
significantly more accurate and orders of magnitude more efficient.
Yet, perhaps more amazing is that the spider eye placement differs
significantly between species!

If everything were hardwired in genetics, this would firstoff present
itself through high differentiation in genetics between species that
have significantly different eye placement.  Secondly, this would
present many problems, as the eye placement changes drastically as a
spider grows and matures.  However, though it may change to some
limited degree as eye placement changes during maturation, the neural
circuitry -- at least the further, more inward neural net layers for
vision and object recognition -- should not change to the same degree.

So these algorithms must be flexible.  Ask an AI nerd at Google how
hard this would be, if they were to program an autonomous drone with
multiple cameras and needed to adhere to these requirements.  Adapting
to an unexpected camera postion and allowing for altered camera
position.  While it'd be very difficult to design generic visual and
spatial recognition algorithms to do this, they would be more flexible
and possibly more efficient.  ** *Orders of magnitude more efficient*
**, to emphasize.

Therefore, due to speciation and growth, the algorithms that these
neural networks embody **must also be genericized**, at least to some
degree. Furthermore, to satisfy the dependency of generic eye
placement and the constraint of minimalized genetic variation between
species, the visual processing system for various species must provide
similar data processing capabilities.  They must be embodied by
functionally similar networks of neurons.  The higher levels of neural
network layers will especially exhibit a more constrained degree of
variation.

IMO.  Keep in mind that I am not citing papers here, as much as I
respect the hard work and dedication that it takes for professional
academics to do so.

<a name="cross-entropy" />

### [Cross Entropy](#cross-entropy)

#### And Other Statistics Concepts I wish I Understood

It appears that neurons and the circuits they form are physical
manisfestations of concepts from statistics.

> Have I blown your mind yet?  That should blow your mind.

Various species of spiders must share certain commonalities in how
their visual processing systems develop.  How does eye placement
affect the development of neural networks in the visual processing
systems of various species.  And if you could somehow develop
specimens of spiders of the same species with various eye placements,
how would that affect the development of those visual processing
neural networks?

This hypothesis of course assumes that you can both measure the state
of neural networks without destroying them *and* measure how the
change in that state is functionally coupled to the differences in eye
placement.

I would surmise that the networks would start off initially the same,
yet the outermost neurons would would gradually diverge.  These
outermost neurons are the ones responsible for initial data
processing.  These are the neurons the would differ the most.  The
innermost neurons may change slightly, yet they would embody mostly
the same information processing structure.  That's my hypothesis.

<a name="visual-processing-au-naturale" />

### [Visual Processing, Au Naturale](#visual-processing-au-naturale)

As for a spider's visual processing, there's neural input into the
visual processing center for each eye.  Then these neural inputs from
each eye would pass data into subsequent neurons.  For example:

#### &#x2605; Constrast maps for each image

#### &#x2605; Disparity maps between image

#### &#x2605; Color differentiation maps, if the ocular hardware is capable of color

These additional layers comprise the initial aggregate information
constructed from visual input.  And then, aggregate information is
built on top of aggregate information. It becomes increasingly complex
and further diverged from the structure of the input input.  The
additional layers might encode information like contrast maps for
composite images.

These layers encode progressively higher order information as they
begin to correlate input to learned phenomena:

#### &#x2605; Static objects

#### &#x2605; Static object types

#### &#x2605; Objects in motion

#### &#x2605; Motion patterns of currently observed objects

#### &#x2605; Patterns of motion of known object-types

<a name="quantum-clarification" />

### [Quantum Clarification](#quantum-clarification)

Basically, these neural networks will repeatedly *differentiate* and
*statistically operate upon* both the input data and aggregate data.
This process is not digital.  It's not analog.  It's chemical.  And
possibly quantum -- in the physics sense, as well as the countable
sense.

Ion size & charge are significant.  In human neurons, neurons work
with mostly 4 ions: Sodium, Magnesium, Potassium and Calcium.  If
you're familiar with the periodic table, you'll notice that these for
ions can be laid out in a two by two grid, distinguished by ion size
and charge.  Sodium and Potassium ions both have +1 charge, whereas
Magnesium and Calcium both have +2 charge.  In digital electronic,
circuits can carry only electrons, with a -1 charge.

So already, we may have a phenomenon that is quantum in the quantum
computing sense.  And even though it may appear that electrical charge
carries signals in the brain, if you look at a much lower level, i
believe you'll find that these electrical discharges are like
aggregate sums of the +1 and +2 charges.  It would be impossible to
completely decode these signals without observing how the ions are
flowing in neurons.

Furthermore, these charge potentials are created by +1 and +2 ions
flowing in and out of neurons through *voltage-gated* ion channels, as
well as other ion channels. These ion channels are often mediated by
neurotransmitters.  These channels are specific to ion size: that is,
an opening for Magnesium or Sodium ions, may have a voltage
differential that *could* pull Potassium and Calcium ions across, if
they were just small enough to fit through the opening.  The ions
would need to be locally available as well.

The point is that, in addition to charge, ion size must have some kind
of significance.  So, AFAIK -- and this is where my specific knowledge
of the subject may fail me -- ion channels for Potassium and Calcium,
from Periodic Table Row 4, would likely also allow the Row 4 elements
Sodium and Magnesium to pass.

So therefore, when ion size is considered with ion charge, *it would
seem* that, instead of electrons encoding a binary value as seen in
digital electronics -- they are the aggregate result of a more
complicated encoding scheme.  And because these information
manipulation processes are encoded in this way -- where electric
charges are more than binary and due to the stochastic mechanisms of
the local space under which these ions operate -- that neural circuits
are orders of magnitude more efficent than similar algorithms on
digital computers.

This input data and aggregated data is continuously recombinated.
Inferences are backpropagated through the neural nets to discover &
reinforce the most useful data processing combinations & methods.
This process of propagating inferences balances neural flexibility
with efficiency, I think.  I believe those two concepts would need to
be balanced: the more complicated the networks, the less efficient
they are and the more flexibilty is allowed.

The spider also needs to be capable of discerning the effect of its
motion with respect to its visual input.  This functionality allows a
spider to make inferences about its environment.  It's essential for
object recogntion, which must be capable of working independently of
the visual frame of reference.  However, though visual processing and
object recognition are isolated to some degree and they are
independent -- they must also be interdependent.

<a name="spidey-time" />

### [Spidey Time](#spidey-time)

At some point, there is a temporal component induced into the visual
processing system. Adding the temporal dimension significantly
complicates the neural networks required, i think.  And the degree to
which temporal information components affect the structure of outer
neurons is likely much greater than the degree to which the inner
components are affected.  Again, i'm refering to the neurons closest
to the input as the outer neurons.

And so, this would mean that time -- as experienced by the spider as
opposed to time as signaled by neurons -- is less tightly coupled to
the higher-level inner components.  I hope I just blew your mind
again.

So this temporal component in the outer neural layers allows the
spider's subconscious networks to better infer how its actions and how
the change in state of its environment will affect those networks'
"perception" of themselves ...  if that makes sense. In other words,
the introduction of temporal components to neural networks, while
significantly complicating them, also expand their functionality,
while making them more efficient.

This underscores a very important point about the nature of
information and knowledge:

> The more information and knowledge you have, the easier it becomes to:
>
> (1) acquire additional knowledge
>
> (2) determine the accuracy of inferences based on your existing knowledge
>
> (3) increase the efficiency of #1 and #2

That should both enlighten you *and* scare the fuck out of you, when
taken in the context of generalized AI.

So, for a spider, it must compare its current visual input state with
that of its memory of previous experiences: long term, short term and
immediate short term.  Instead of being divided into three distinct
categories, this is moreso a continuoum of long term & short term.

So, for an example of immediate short term temporal processing, data
must be fed back into the neural network.  This would provide
information on the spider's motion with respect to it's field of
vision or about objects in motion.

Short and long-term information is likely fed back into the neural
network at a higher level.  Therefore, the "longer-term" the
experiences are, the more likely this is encoded into "inner" neural
networks.  It would be moreso involved into higher level pattern
recognition and processes.  This concept could be extended into
infinity -- asi am wont to do -- and i would make the assumption that
the most important knowledge is that which transends time.  But I
think I skipped a few steps there, lulz.

So, the medium short-term processing could provide information on
patterns of objects in motion.  Some of these patterns would need to
be specific for object types.  That is, the spider needs to recognize
that large objects moving quickly towards it are threatening.  And
small objects moving towards it might"e food.

Long term information is used to make inferences about higher order
information, like patterns & behaviors of objects within it's
environment.  Or behaviors, like feeding, web-making, etc.

Some of these higher-order patterns need to map to connections to
phenomena imprinted by instinct.  The phenomenon of food or fear.  So
there must be some mechanism whereby a spider innately recognizes
large objects moving quickly towards it as dangerous, without the need
of experience to induce a fearful response. I believe that instinct is
moreso what causes the spider to more quickly correlate these
experiences as being positive/negative.  This is in contrast to the
idea that instinct is specific knowledge encoded during gestation. So,
IMO, instinct is a mechanism that enables some experiences to be
categorized more quickly and to be imprinted more strongly.

<a name="do-spiders-dream-of-electric-sheep" />

### [Do Spiders Dream of Electric Sheep?](#do-spiders-dream-of-electric-sheep)

None of what I'm talking about is consciously processed, of course.  I
do believe that all animals share a capacity for consciousness.  I'm
making a ton of assumptions in this article, from an incredibly
limited set of observations.  This is all information that is
represented by cellular networks of neurons, neurotransmitters and
ions.  I'm not saying the spider 'thinks' about this.  Or that the
spider somehow philosophizes about phenomena in its environment to
make useful insights.  I'm not sure if the automatic nature of this
makes it *more* or *less* amazing.

I'm leaning towards more amazing. To me, it is simply astounding that
such a small animal with a microscopic brain, virtually incapable of
thought, is somehow capable of producing a such a vast spectrum of
information about their environment.  Their capacity for pattern
recognition crucial to their survival, the degree to which it is
efficent, and the immeasurable difference to our own capacity for AI
-- fascinating. Furthermore, they are capable of doing this using
*only* input from their environment.

And with virtually zero communication: I say that because *every
interaction* -- whether using language or via non-typical interaction
with another species -- requires a certain degree of communication in
the form of metainformation exchange.  In other words, every
interaction carries a baseline level of information -- and it is
critical -- as a kind of meta-information. So, basically, the
information 'exchanged' through one species input neurons to another
species input neurons -- it IS communication.  It might not be
symbolic communication.  We might not recognize it as such, but it is.

And, for further astonishment, these cellular networks autonomously
adapt to provide aggregate information based on inputs to each neural
node.  This reinforces encoded information that seems correlated to
previously recognized patternr or has shown to be useful in predicting
the state of the spider's environment.  And it must do this effectivly
enough to avoid a fatal mistake **before reproduction**. These neural
networks must be capable of combinating and recombinating all the
input data.

<a name="ad-nihilum" />

### [Ad Nihilum](#ad-nihilum)

Anyway, as you can clearly see, a limited amount of information will
often set my mind off on a chain reaction of inferences.  It's pretty
fucking annoying honestly.  I have lots of shit to do.  What is
frustrating is that this happens because i'm intelligent AND i don't
have the answers. There's many subjects I spend an inordinant amount
of time thinking about, where I wouldn't have to expend the effort if
I just knew this answers.

This underscores the ironinc dichotomy of becoming smart and becoming
intelligent.  College makes you smart.  That's the correct path, the
best path for most people.  But honestly, it is the constant and
painful struggle which has made me intelligent.  It is that I DO NOT
KNOW, which makes me struggle to learn.  And because I have struggled
to learn on my own -- and wasted so much time doing so -- this is why
I'm able to make such insights.  However, if everyone took this path,
our systems of knowledge propagation would crumble.  And my own blind
spots are glaring.  It's often pretty embarrassing what I don't know.

> "For years, I thought white eggs were white because they went
> through a bleaching process ... LMAO ok that's embarrassing." - Me,
> in 2015, after several years of living in a society where it was
> apparently illegal to tell me to tie my shoes.

If I went to college, I would have these answers.  The system of
information processing and structure of knowledge in my mind would
look just like everyone else's.  I wouldn't have the need to search so
hard.  I also would lack my propensity for creativity, but the number
of times i have 'discovered' something that already exists both amazes
me and embarrasses me.

If I was smarter and if my mind embodied a more similar structure of
knowledge to that of the college educated, I would be much more
capable of communicating these ideas. Because I didn't learn most of
this in college, I tend to develop my own vocabulary to describe what
I'm talking about.  And because most college educated people are used
to talking about their domain of expertise using a limited set of
vocabulary, I often find myself getting tuned out, since I'm not using
the right words and I don't have the same understanding of
distinctions between ideas -- but what if some of those distinctions
in the commonly understood structure of knowledge were inaccurate,
misleading or limiting?

And finally, if I did excel in college -- i fell through the cracks,
truly I didn't even try -- then I would have been understood to be
extremely smart.  I would have been whisked away at some inordinately
expensive project lasting years.  I would narrowly focus on one
subject and perhaps waste my life.  I hate to say this, since -- as
much as I have issues with the academic system -- I also believe it
has great value and I do not want to demonstrate the wrong path for
others.

### Peace
